---
title: "Midterm Project"
author: "Yihong Hu, Sabir Nazarov, Will Friedrichs"
date: "10/20/2021"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
    runtime: shiny
# >>>>>>> 05ab802ca33d750c6faefee82b91766e0f572e3e
---

# Introduction

Zillow's housing market predictions are an integral part of its business model, helping the company achieve a greater understanding of how the market will value properties. Our team is confident that through our geospatial machine learning-based model that considers not only attributes of homes, but local factors as well, we can improve Zillow's house price predictions for the Boulder County study area and provide a template which can be adapted to other localities.

This project outlines and analyzes the process by which we created our model in three broad stages: data gathering, regression modeling, and finally, prediction and validation. Our process involves splitting our total dataset into a training set consisting of 75% of the points, and a testing set consisting of the other 25%. These data are used to create our model, which predicts the 100 observations of the challenge set.

_We found_

_The chunk below, for example, loads 17 libraries for use later, helps set up the generation of the r markdown file, gives access to online resources helpful to our analysis, and sets up a color scheme to be used for data visualization. I'll use this for comments_

```{r The Setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidycensus)
library(sf)
library(kableExtra)
library(dplyr)
library(ggcorrplot)
library(caret)
library(spdep)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(jtools)     
library(ggstance)
library(rpart)
library(ggplot2)
library(stargazer)
library(viridis)

# set up knitr
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	include = TRUE,
	cache = TRUE,
	echo=TRUE
)

# Set root directory
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

# Locate the source of functions from raw.githubusercontent.com
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

# Save q5 and qbr functions for later use
q5 <- function(variable) {as.factor(ntile(variable, 5))}

qbr <- function(df, variable, rnd) {
  if (missing(rnd)) {
    as.character(quantile(round(df[[variable]],0),
                          c(.01,.2,.4,.6,.8), na.rm=T))
  } else if (rnd == FALSE | rnd == F) {
    as.character(formatC(quantile(df[[variable]]), digits = 3),
                 c(.01,.2,.4,.6,.8), na.rm=T)
  }
}

# Set a color palette for later use
palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")
```


# Data Gathering

### 2019 ACS Data

The data gathering process begins by accessing tract-level geography, population, and demographic data from the 2019 American Community Survey (ACS). Variables of interest are isolated and wrangled to create features required for the analysis.

```{r ACS Data, results = FALSE, message=FALSE,warning=FALSE, cache = FALSE}
varlist_2019 <- load_variables(2019, "acs5", cache = TRUE)

census_api_key("94efffd19b56ad527e379faea1653ee74dc3de4a",overwrite = TRUE)

tracts19 <- get_acs(geography = "tract",                         
                    variables = c("B01001_001","B23025_004","B06011_001",
                                  "B06012_002","B02001_002","B25002_003",
                                  "B25013_006","B08013_001","B15012_009"), 
                    year=2019, 
                    state=08, 
                    county=013,
                    output = "wide",
                    geometry=TRUE) %>% 
            st_transform('ESRI:102254') %>%
            select( c("GEOID","B01001_001E","B23025_004E","B06011_001E",
                      "B06012_002E","B02001_002E","B25002_003E",
                    "B25013_006E","B08013_001E","B15012_009E","geometry") ) %>%
            rename(tot_pop = "B01001_001E",
                   empl_pop = "B23025_004E",
                   med_inc = "B06011_001E",
                   pvty_pop = "B06012_002E",
                   white_pop = "B02001_002E",
                   vac_occ = "B25002_003E",
                   own_occ_bach = "B25013_006E",
                   tt_work = "B08013_001E",
                   sci_bach = "B15012_009E") %>%
            mutate(area = as.numeric(st_area(geometry)/1000000))%>%
            mutate(pop_den = tot_pop/area)

```

### Primary Training Dataset: Housing Data

The housing data, provided as part of the project requirements, provides housing prices along with a variety of features associated with houses. After other variables are added to the dataset, we have the features we need to create our model.

We then joined ACS demographic data that we theorized could predict housing prices. Each house is ascribed the demographic variables associated with the tract in which it is located.

```{r Project Data, results=FALSE, warning =FALSE}
# This loads all the data into "studentData".
studentData <- st_read("studentData.geojson", crs = 'ESRI:102254')%>%
  mutate(Age = 2021 - builtYear) %>%
  mutate(TotalBath = nbrThreeQtrBaths + nbrFullBaths + nbrHalfBaths)

studentData %>%
st_make_valid(geometry)

# Attach ACS data
studentData <- st_join(studentData, tracts19, join = st_within)

#Boulder County Boundary
BoulderCounty_Bundary <-
st_read("https://opendata.arcgis.com/datasets/964b8f3b3dbe401bb28d49ac93d29dc4_0.geojson")%>%
  select(geometry) %>%
  st_transform('ESRI:102254')

#Boulder Municipal Boundary
BoulderMuni_Boundary <-
  st_read("https://opendata.arcgis.com/datasets/9597d3916aba47e887ca563d5ac15938_0.geojson")%>%
  st_transform('ESRI:102254')%>%
  rename(Municipality = ZONEDESC)
  
```

### Municipalities

Boulder County and the boundaries of the municipalities within are drawn from open-source geometry files. Fig. 1 below displays the sale price of homes as raster data, superimposed onto Boulder County and its 10 municipalities.

```{r Project Data Map, Message = FALSE, warning = FALSE}
#Map of Boulder County with Housing Sales Price and municipal boundary
ggplot()+
  geom_sf(data = BoulderCounty_Bundary, fill = "grey70") +
  geom_sf(data = BoulderMuni_Boundary, aes(fill = Municipality, alpha=0.5),colour = "white") +
  geom_sf(data = studentData, aes(colour = q5(price)),size=.85)+
  labs(title = "Housing Sales Under Municipalities", subtitle = "Boulder County, CO", caption = "Fig. 1")+
  scale_colour_manual(values = palette5,
                      labels=qbr(studentData,"price"),
                      name = "Sale Price") +
  mapTheme()

# Median Housing Price in Each Municipality
House_in_muni_boundary <- st_intersection(studentData, BoulderMuni_Boundary) %>%
  mutate(pct_pvty = pvty_pop/tot_pop * 100,
         pct_white = white_pop/tot_pop * 100,
         )

```

Table 1, below summarizes key indicators by municipality. Each variable represents the mean values ascribed to homes in the municipality. For example, poverty rate represents not the poverty rate of the municipality, but the poverty rate ascribed to each house in that municipality. While the rates are unweighted averages, we conclude that house price is strongly related to municipality. Boulder City consistently has the highest housing prices, a trend that we felt should be reflected in our model.

```{r Project Data Table}

Municipality.Summary <-
  st_drop_geometry(House_in_muni_boundary) %>%
  group_by(Municipality) %>%
  summarize(Medium.Price = median(price, na.rm = T),
            Mean.Price = mean(price, na.rm = T),
            Population = mean(tot_pop, na.rm = T),
            "Poplation Density per km^2"= mean(pop_den,na.rm=T),
            "Median Income" = mean(med_inc, na.rm = T),
            "Poverty Rate" = mean(pct_pvty, na.rm = T),
             "White Poplation in %" = mean(pct_white, na.rm = T),
            "Building Age" = mean(Age,na.rm = T)) %>%
  arrange(desc(Medium.Price))

kable(Municipality.Summary, digits = 2) %>%
  kable_styling() %>%
  footnote(general_title = "\n",
           general = "Table 1. Housing prices grouped by municipalities")


studentData <- st_join(studentData, BoulderMuni_Boundary, join = st_within)

studentData <- 
studentData %>%
  select(-c(OBJECTID, ZONECLASS, LASTUPDATE,LASTEDITOR, REG_PDF_URL, Shape_STArea__, 
            Shape_STLength__, ShapeSTArea,ShapeSTLength
            ))

# Creating dummy variables for municipalities
studentData <- mutate(studentData, Loui_dummy = case_when(Municipality=="Louisville"~ 1, Municipality!="Louisville"~ 0))
studentData <- mutate(studentData, Ward_dummy = case_when(Municipality=="Ward"~ 1, Municipality!="Ward"~ 0))
studentData <- mutate(studentData, Jame_dummy = case_when(Municipality=="Jamestown"~ 1, Municipality!="Jamestown"~ 0))
studentData <- mutate(studentData, Nede_dummy = case_when(Municipality=="Nederland"~ 1, Municipality!="Nederland"~ 0))
studentData <- mutate(studentData, Boul_dummy = case_when(Municipality=="Boulder"~ 1, Municipality!="Boulder"~ 0))
studentData <- mutate(studentData, Erie_dummy = case_when(Municipality=="Erie"~ 1, Municipality!="Erie"~ 0))
studentData <- mutate(studentData, Lafa_dummy = case_when(Municipality=="Lafayette"~ 1, Municipality!="Lafayette"~ 0))
studentData <- mutate(studentData, Long_dummy = case_when(Municipality=="Longmont"~ 1, Municipality!="Longmont"~ 0))
studentData <- mutate(studentData, Lyon_dummy = case_when(Municipality=="Lyons"~ 1, Municipality!="Lyons"~ 0))
studentData <- mutate(studentData, Supe_dummy = case_when(Municipality=="Superior"~ 1, Municipality!="Superior"~ 0))

studentData$Loui_dummy[is.na(studentData$Loui_dummy)] <- 0
studentData$Ward_dummy[is.na(studentData$Ward_dummy)] <- 0
studentData$Jame_dummy[is.na(studentData$Jame_dummy)] <- 0
studentData$Nede_dummy[is.na(studentData$Nede_dummy)] <- 0
studentData$Boul_dummy[is.na(studentData$Boul_dummy)] <- 0
studentData$Erie_dummy[is.na(studentData$Erie_dummy)] <- 0
studentData$Lafa_dummy[is.na(studentData$Lafa_dummy)] <- 0
studentData$Long_dummy[is.na(studentData$Long_dummy)] <- 0
studentData$Lyon_dummy[is.na(studentData$Lyon_dummy)] <- 0
studentData$Supe_dummy[is.na(studentData$Supe_dummy)] <- 0
```

### Parks, Landmarks, and Trail Heads

The Boulder area is rich in natural beauty and outdoor activities. We thought it likely that home buyers might place a premium on proximity to locations like parks, natural landmarks, and Trail Heads, and engineered features accordingly.

For parks, we downloaded a polygon of park area in Boulder County, and created three variables based on each home's "nearest neighbors". One variable represented the shortest distance between a home and park space, another represented the mean of the distances to the two nearest areas of park space, and the third took the mean distances to the three nearest park spaces. Our fourth variable was created by the number of park centroids within a 500m buffer of each home. A map of trail heads produced a similar set of variables. A 1000m buffer was used for the last variable rather than a 500m buffer because trail heads are disproportionately accessed by cars. For landmarks, a single variable was added, expressing the distance between the home and the nearest landmark.

```{r Parks & Landmarks, results=FALSE, warning=FALSE}
# Load Park data
GreenSpacePolygon <- st_read("County_Open_Space.geojson") %>%
                     st_transform('ESRI:102254')

Park <- GreenSpacePolygon[!is.na(GreenSpacePolygon$PARK_GROUP),] %>%
        st_centroid()
  
st_c <- st_coordinates


studentData <-
  studentData %>% 
  mutate(park_nn1 = nn_function(st_c(studentData), st_c(Park), 1),
         park_nn2 = nn_function(st_c(studentData), st_c(Park), 2),
         park_nn3 = nn_function(st_c(studentData), st_c(Park), 3),
         park_dist = st_distance(studentData,Park))

Park_in_buffer <-
  st_join(Park,st_buffer(studentData,500),join = st_within)

PKCount <-
  Park_in_buffer %>%
  count(MUSA_ID)%>%
  st_drop_geometry()

studentData <-
  left_join(studentData, PKCount, by = "MUSA_ID" )

studentData <-
  studentData %>%
  rename(PKCount500m = n)

studentData$PKCount500m[is.na(studentData$PKCount500m)] <- 0

# Load Landmarks data
landmarksPolygon <- st_union(st_read("Natural_Landmarks.geojson")) %>%
  st_transform('ESRI:102254')

# attach distance to land marks data
studentData <- mutate(studentData, landmark_dist = st_distance(studentData, landmarksPolygon))

# Loading Trail Heads Locations
TrailHead <- 
  st_read("https://opendata.arcgis.com/datasets/5ade4ef915c54430a32026bcb03fe1d7_0.geojson") %>%
  st_transform('ESRI:102254')%>%
  select(geometry)

#Apply buffer counts and nearest neighbor function on trail heads
Trailhead_in_buffer <-
  st_join(TrailHead,st_buffer(studentData,1000),join = st_within)

TrailHeadCount <-
  Trailhead_in_buffer %>%
  count(MUSA_ID)%>%
  st_drop_geometry()

studentData <-
  left_join(studentData, TrailHeadCount, by = "MUSA_ID" )

studentData <-
  studentData %>%
  rename(TrailHead1000m = n)

studentData$TrailHead1000m[is.na(studentData$TrailHead1000m)] <- 0

studentData <-
  studentData %>% 
  mutate(
    head_nn1 = nn_function(st_c(studentData), st_c(TrailHead), 1),
    head_nn2 = nn_function(st_c(studentData), st_c(TrailHead), 2), 
    head_nn3 = nn_function(st_c(studentData), st_c(TrailHead), 3),
    head_nn4 = nn_function(st_c(studentData), st_c(TrailHead), 4), 
    head_nn5 = nn_function(st_c(studentData), st_c(TrailHead), 5))

studentData <-
  studentData %>%
  mutate(trail_dist = st_distance(studentData, TrailHead))
```


### Playgrounds, Schools, Flood Plains

Additional variables to be considered by the model come from proximity to playgrounds, schools, and floodplains. Playgrounds are important amenities for families with young children which we theorize is a large portion of the home buying contingency.  Our playground indicator reflects the number of playgrounds within a 500m radius of the house.  

Similarly, we know that availability of quality schools contributes to higher house prices.  Though no school quality data is readily available for Boulder county we use a 500m radius indicator as a proxy. We also engineered features related to private schools using nearest neighbor function.  

Finally, distance to the floodplain is added because of the potential connection between risky proximity to floodable areas and disadvantaged areas. We use a distance between a given home and the floodplain as a variable in our model. However, the model can benefit from a more granular, risk-management feature related to flooding.  If similar analysis is replicated in a different locality we recommend checking insurance databases for availability of this data.

```{r Playgrounds & Schools, results=FALSE, warning=FALSE}
#Loading Playground Locations
Playground <- 
  st_read("Playground_Sites_Points.GEOJSON") %>%
  st_transform('ESRI:102254')%>%
  drop_na(PROPID)%>%
  select(geometry)

Playground.sf <-
  st_join(Playground,st_buffer(studentData,500),join = st_within)

PGCount <-
  Playground.sf %>%
  count(MUSA_ID)%>%
  st_drop_geometry()

studentData <-
  left_join(studentData, PGCount, by = "MUSA_ID" )
  
studentData <-
  studentData %>%
  rename(pgcount500m = n)

studentData$pgcount500m[is.na(studentData$pgcount500m)] <- 0
  

#Loading School Locations
Schools <- 
  st_read("CDPHE_CDOE_School_Locations_and_District_Office_Locations.GEOJSON")%>%
  st_transform('ESRI:102254')%>%
  filter(COUNTY == "BOULDER")
  
Private_School <-
  filter(Schools, startsWith(Type_, "Non-"))

studentData <-
  studentData %>% 
  mutate(
  school_nn1 = nn_function(st_c(studentData), st_c(Schools), 1))

studentData <-
  studentData %>%
  mutate(
    privatesch_nn1 = nn_function(st_c(studentData),st_c(Private_School),1),
    privatesch_nn2 = nn_function(st_c(studentData),st_c(Private_School),2),
    privatesch_nn3 = nn_function(st_c(studentData),st_c(Private_School),3),
    privat_dist = st_distance(studentData,Private_School, by_element = TRUE))

#Loading Flood Plain
Floodplain <-
 st_read("https://opendata.arcgis.com/datasets/30674682e55e4e1b8b0e407b0fe23b9a_0.geojson") %>%
 st_transform('ESRI:102254')%>%
  st_union()

studentData <-
  studentData %>%
  mutate(flood_dist = st_distance(studentData, Floodplain, by_element=TRUE))


# This selects all numeric variables as preparation for the
# correlation analysis that follows.
#(also delete the outliar)

cleanData <- 
  select_if(st_drop_geometry(studentData), is.numeric) %>%
  select(!c(year, ExtWallSec, IntWall, Roof_Cover, Stories, UnitCount))%>%
  slice(-2638)

studentData.GEOID <-
  select(studentData, GEOID, MUSA_ID)

cleanData <-
  left_join(cleanData, studentData.GEOID)

cleanData$Ac[(cleanData$Ac)== NA] <- 0
cleanData$Heating[(cleanData$Heating) == NA] <- 0
```

Fig. 1.2, 1.3, 1.4 map some variables in Boulder County. Most of the parks and trail heads are located outside of the residential areas. Most of the private schools are located in Boulder City and Longment. From a glance, housing sales are very related to private schools.

```{r Maps of Indepdent Variable, message = FALSE, warning=FALSE}
#Parks
ggplot() + 
  geom_sf(data = BoulderCounty_Bundary, fill = "grey60") +
  stat_density2d(stat = "sf_coordinates",
                 data = data.frame(st_coordinates(Park)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_viridis(name = "Density Level") +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
   geom_sf(data = studentData, aes(colour = q5(price)),size=.85)+
  scale_colour_manual(values = palette5,
                      labels=qbr(studentData,"price"),
                      name = "Sale Price") +
  labs(title = "Density of Parks with Housing Sales", caption = "Fig 1.2") +
  mapTheme(title_size = 14)

#Trail Head Location
ggplot() + 
  geom_sf(data = BoulderCounty_Bundary, fill = "grey60") +
  geom_sf(data = TrailHead, colour = "purple") +
   geom_sf(data = studentData, aes(colour = q5(price)),size=.85)+
  scale_colour_manual(values = palette5,
                      labels=qbr(studentData,"price"),
                      name = "Sale Price") +
  labs(title = "Trail Head Location with Housing Sales", caption = "Fig 1.3") +
  mapTheme(title_size = 14)

#Private School
ggplot() + 
  geom_sf(data = BoulderCounty_Bundary, fill = "grey60") +
  stat_density2d(stat = "sf_coordinates",
                 data = data.frame(st_coordinates(Private_School)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_viridis(name = "Density Level") +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
   geom_sf(data = studentData, aes(colour = q5(price)),size=.85)+
  scale_colour_manual(values = palette5,
                      labels=qbr(studentData,"price"),
                      name = "Sale Price") +
  labs(title = "Density of Private School with Housing Sales", caption = "Fig 1.4") + mapTheme(title_size = 14)
```
# Predictions and Validation

### Preparation for machine learning and regression model

The model creates coefficients for several of the previously mentioned variables along with an additional constant value. To make a price prediction for a home, the values of the home's variables (square footage, population of the census tract, distance to landmarks, etc.) are multiplied by the coefficients created by the model, and the sum of the results produces a house price. Our OLS model must first look at the relationships in the training set data so that it can optimize the coefficients and produce the best possible model. 

To test our model, we use a 75/25 split of the data. This means that 75% of our model will be trained on, and 25% of the model will be tested on. Since we already know the price of the points we are testing, we get a sense of the accuracy of our model before we make our predictions for the challenge set.

```{r Train & Test, results = FALSE, warning=FALSE}
# The test data only includes rows comprising the test set.
# Remove Outliars

test.Data <- filter(cleanData, toPredict == 1)

#Split the âtoPredictâ == 0 into a separate training and test set 
#using a 75/25 split

train.Data <- filter(cleanData, toPredict == 0) %>%
  na.omit()
  

inTrain <- createDataPartition(
  y = paste(train.Data$Heating, train.Data$Ac,train.Data$GEOID), 
  p = .75, list = FALSE)
train.Data.1 <- train.Data[inTrain,] 
train.Data.2 <- train.Data[-inTrain,]

#Dataframe with variables of interest
train.Data.clean <-
  train.Data %>%
  select(price, 
         section_num,
         qualityCode,
         TotalFinishedSF,
         Age,
         Ac,
         Heating,
         med_inc,
         nbrRoomsNobath,
         vac_occ,
         tot_pop,
         pop_den,
         pvty_pop,
         head_nn5,
         park_nn1,
         Boul_dummy,
         Loui_dummy,
         pgcount500m,
         TotalBath,
         flood_dist,
         landmark_dist,
         privat_dist)
```

Fig. 2 shows a graph of the relationship between Housing Price and the distance to the single nearest trail head neighbor.

```{r Trailheads, warning = FALSE, echo=FALSE}
# This function here attempts to fit a linear model to
# the relationship between "testData" variables.
ggplot(data = train.Data, aes(head_nn5, price)) +
       geom_point(size = .5) + 
       geom_smooth(method = "lm")+
  labs(title = "Relationship between Housing Price and the Average Distance to the Nearest Five Trail Heads", x = "Distance (m)", y = "Sale Price ($)",caption = "Fig. 2")+
  plotTheme()
```

### Correlation & Test Signifcance

Prior to the the regression, the relationships across all the variables are evaluated to test the levels of significance of their relationship with each other. This process determines if variables are colinear, meaning that two or more variables give such similar information that to include all of them would be redundant. In Fig. 3, Pearson Correlation is imputed: a strong orange shows strong positive correlation and a strong green shows strong negative correlation between selected variables. 

Note that price is strongly correlated with section number, quality code, total finished square footage, number of bathrooms, location within or not within Boulder City, and count of playgrounds within a 500 meter radius. 

```{r Correllation & Test Signicance, warning=FALSE, echo = FALSE}

# Correlation analysis: pearson for each relationship
ggcorrplot(
  round(cor(train.Data.clean), 1), 
  p.mat = cor_pmat(train.Data.clean),show.diag = TRUE,
  colors = c("#25CB10", "white", "#FA7800"),
  type="lower",
  insig = "blank") +  
  labs(title = "Correlation across numeric variables",caption = "Fig. 3") 
```

Below is a in-depth look of Fig. 4, presenting correlations between price and four selected variables. There is a noticeably strong positive correlation between price and total square footage. Negative correlations are also clear between price and the distances to landmarks and private schools.

```{r Scatter Plots, echo=FALSE, message=FALSE}
#Scatter plots between price and variables
cleanData %>%
  select(price,"Playground Count in 500m" = pgcount500m, 
         "Total Squrefootage" = TotalFinishedSF,"Distance to Landmark (m)" = landmark_dist,
         "Distance to the nearest private school" = privat_dist)%>%
gather(Variable, Value, -price) %>% 
  ggplot(aes(Value, price)) +
  geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
  facet_wrap(~Variable, ncol = 3, scales = "free") +
  labs(title = "Price as a function of continuous variables", caption = "Fig. 4") +
  plotTheme()

# This function allows for the plug-in of variables from "studentData".
testSignificance <- lm(price ~ ., data = train.Data %>% 
                    dplyr::select(price, 
                                  qualityCode,
                                  TotalFinishedSF,
                                  mainfloorSF,
                                  Age,
                                  pop_den,
                                  med_inc
                                  ))

# This gives us our r-squared value, which measures fit to the training data.
summary(testSignificance)

```


### 100-fold validation

The next step of the project involves cross-validation using k-folds.  We chose 100-fold cross-validation which involves randomly partitioning the training set into 100 groupings.  We trained the model using 99 groupings and checked the errors on the remaining group. During the research process, we tested many combinations of variables, but this combination seemed reasonable and was particularly successful in making predictions.  The result shows approximately 70% of the variance is explained by the model (depending on the iteration).  Using this result we proceed to applying our predictions to the challenge set.

Table 3. displays the p-value, a “score” that describes how well each variable performs in the prediction of house price across all of the folds of the 100-fold cross validation. A p-value less than 0.05 for a variable indicates that variable is significant. The R-squared value displayed below gives a percentage of how much of the total error our model accounts for.


```{r P-Value Table, results = 'asis'}

# Multivariate regression 
reg1 <- lm(price ~ ., data = train.Data.1 %>% 
             dplyr::select(price, 
                 section_num,
                 qualityCode,
                 TotalFinishedSF,
                 Ac,
                 Heating,
                 Age,
                 med_inc,
                 landmark_dist,
                 nbrRoomsNobath,
                 pvty_pop,
                 privat_dist,
                 head_nn5,
                 park_nn1,
                 Loui_dummy,
                 Nede_dummy,
                 Boul_dummy,
                 Erie_dummy,
                 Long_dummy,
                 Lyon_dummy,
                 pgcount500m,
                 TotalBath,
                 flood_dist,
                 privat_dist))



stargazer(reg1, type = "html", title = "Table 3. Variation of selected variable and error control of the regression model",out="table1.html",dep.var.labels = "Housing Price")

```

```{r}
reg1.tidy <-
  tidy(reg1,
       "Section Number" = section_num)

kable(reg1.tidy, digits = 2, caption = "Table 4. Regression model predicting variation on housing prices", 
      col.names = c("Predictor","Estimate","Standard Error", "T-Value","P-Value")
      ) %>%
  kable_styling()
```      
       
    

```{r 100 k-fold}
# This sets up k-fold cross-validation.
k = 100
fitControl <- trainControl(method = "cv", number = k)
set.seed(825)
# variables in the "select(...)" function are considered in the analysis here.
regression.100foldcv <- 
  train(price ~ ., data = train.Data.1%>% 
          select(price, 
                 section_num,
                 qualityCode,
                 TotalFinishedSF,
                 Ac,
                 Heating,
                 Age,
                 med_inc,
                 landmark_dist,
                 nbrRoomsNobath,
                 pvty_pop,
                 privat_dist,
                 head_nn5,
                 park_nn1,
                 Loui_dummy,
                 Nede_dummy,
                 Boul_dummy,
                 Erie_dummy,
                 Long_dummy,
                 Lyon_dummy,
                 pgcount500m,
                 TotalBath,
                 flood_dist,
                 privat_dist
          ),
  method = "lm", trControl = fitControl, na.action = na.pass)

# The resulting Mean Absolute Error (MAE) of running this line tells us how
# successful our model is at predicting unknown data. 

data = regression.100foldcv

```

### MAE Histogram

While we are proud of our model's ability to make predictions, it of course cannot be perfect. The following histogram displays the distribution of the Mean Average Error (MAE) for each of the folds of the 100-fold cross-validation exercise. MAE measures the magnitude of the average difference between the predicted price and the actual price of the home. In most of the folds, the predictions were around $200,000 off-base on average. 

```{r MAE Histogram, results=FALSE,warning=FALSE,message=FALSE}
#MAE Histogram
#This allows you to see the distribution of MAE of the folds

ggplot(regression.100foldcv$resample, aes(x=MAE))+
  geom_histogram(colour = "white",fill="orange")+
  labs(title="Distribution of MAE",x="Mean Absolute Error", y = "Count",
       subtitle = "k-fold cross-validation; k = 100",
       caption = "Fig. 5")+
  theme_apa()

```

# Test Set Prediction

After a set of regression coefficients have been produced and tested by 100-fold validation, the model is used to predict price for testing set. The results created by running this model are displayed by Table 6.

The resulting Mean Absolute Percentage Error indicates that the model created a decent output. Predictions were, on average, within 30% of the value of the price. This bodes well for the potential of the model to predict mostly reasonable prices for the challenge set. 

```{r MAE and table, echo=FALSE}
#Run model on unpredicted dataset

train.Data.2 <-
  train.Data.2 %>%
  mutate(SalePrice.Predict = predict(reg1, train.Data.2))
         
train.Data.2 <-
  train.Data.2 %>%
        mutate(SalePrice.Error = SalePrice.Predict - price,
         SalePrice.AbsError = abs(SalePrice.Predict - price),
        SalePrice.APE = (abs(SalePrice.Predict - price)) / SalePrice.Predict)

#Table of mean absolute error and MAPE
Prediction.Variation <- "Trial 1" 
MeanAb <- mean(train.Data.2$SalePrice.AbsError,na.rm = T)
MeanAPE <- mean(train.Data.2$SalePrice.APE, na.rm = T)

MeanAb.table <- data.frame("Model"= Prediction.Variation,"Mean Absolute Error ($)"= MeanAb,
                           "Mean Absolute Percentage Error (%)" = MeanAPE)

kable(MeanAb.table, digit = 2,
      caption = "Table 6. Mean Absolute Error and Absolute
      Percentage Error on the Prediction Trial",
      col.names = c("Regression",
                    "Mean Absolute Error ($)",
                    "Mean Absolute Percentage Error (%)"))%>%
  kable_styling(latex_options = "HOLD_position")
```

### Plot Predictions

Fig. 6 compares predicted price to actual price. The logarithmic relationship shows that for lower prices, our model slightly under-predicts observed prices. Mid-level prices tend to be over-predicted, but the highest _edit_

Fig. 7, showing absolute errors between observed and predicted housing price across Boulder County, demonstrates that our model generally has consistently low absolute error.

```{r Plot Predictions, message=FALSE, warning = FALSE}
#Plot of predicted prices as a function of observed price
ggplot(data = train.Data.2, aes(price,SalePrice.Predict)) +
  geom_point(size = .85,colour = "orange") + 
  geom_smooth(method = "lm",colour = "red",size = 1.2)+
  labs(x = "Obeserved Prices",y = "Predicted Prices", title = "Predicted Prices as a Function of Observed Prices",
       subtitle = "Boulder County, CO (all in $)", 
       caption = "Fig. 6")+
  theme(plot.title = element_text(size = 20),
        plot.subtitle = element_text(size = 15),
        axis.title.x = element_text(size = 15),
        axis.title.y = element_text(size = 15),
        axis.line = element_line(colour = "grey50", size = 1),
        panel.grid.major = element_line(linetype = "dotted",size = 1))

#Map of residuals
#This allows you to see the degree of error across over Boulder County

train.Data.2 <-
  left_join(train.Data.2, tracts19)%>%
  st_as_sf()

ggplot()+
  geom_sf(data = BoulderCounty_Bundary, fill = "grey70") +
  geom_sf(data = tracts19,colour = "Black") +
  geom_sf(data = train.Data.2, aes(colour = SalePrice.AbsError),size=.90)+
  scale_colour_gradient(low = "#00cfbb", high = "#6f5ad3",
    name = "Absolute Error ($)" ) +
  labs(title = "Absolute Errors between Observed and Predicted Housing Price", caption = "Fig. 7")+
  mapTheme()

```

### Lag Price Errors

There is a slight trend relating Sale Price Error to Spatial Lag Errors, suggesting some spatial lag effect. The points with the greatest error are generally a little bit more likely to have nearby points with high error. 

```{r Lag Price Errors, fig.width=8,fig.height=8}
#Lag Price Errors

coords.test <-  st_coordinates(train.Data.2) 
neighborList.test <- knn2nb(knearneigh(coords.test, 5))
spatialWeights.test <- nb2listw(neighborList.test, style="W")

train.Data.2.lag <-
train.Data.2 %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error))

ggplot(data = train.Data.2.lag, aes(lagPriceError, SalePrice.Error)) +
  geom_point(size = .85,colour = "orange") + 
  geom_smooth(method = "lm",colour = "red",size = 1.2)+
  labs(x = "Spatial Lag of Errors (Mean error of five nearest neighbors)",y = "Sale Price Error", title = "Lag Sale Price Errors against Predicted Sale Price Error",
       subtitle = "Boulder County, CO (all in $)",caption = "Fig. 8")+
  theme(plot.title = element_text(size = 20),
        plot.subtitle = element_text(size = 15),
        axis.title.x = element_text(size = 15),
        axis.title.y = element_text(size = 15),
        axis.line = element_line(colour = "grey50", size = 1),
        panel.grid.major = element_line(linetype = "dotted",size = 1))
```



### Moran's I

Moran's I is a measure that assess  spatial autocorrelation: specifically, whether values are spatially clustered, random, or dispersed. The Moran's I of our error is calculated below, and compared with the frequency of 999 randomly permutated I. 

A I approaching -1 indicates greater dispersal, while an I approaching 1 indicates clustering. Random spatial distribution is indicated by a I of 0. 
Because the spatial distribution of the randomly permutated I is random, its Moran's I is very close to zero. The observed Moran's I is higher than all 999 of the permutated Is, confirming the existence of a spatial autocorrelation. 

```{r Morans I, echo = FALSE, warning = FALSE}
#Moran's I

moranTest <- moran.mc(train.Data.2$SalePrice.Error, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and Permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count",
       caption = "Fig. 9") +
  plotTheme()

```

### Predicted Results by Municipality

The map below shows predicted values for the entire dataset overlaid onto a map of municipalities. According to the model, most of the municipalities are predicted to have houses at a range of prices. The City of Boulder is interestingly more clustered, with higher priced houses in the north and comparatively lower priced houses in the south. 

_edit_

```{r Prediction on Challenge Set, echo=FALSE, warning=FALSE, message=FALSE}
#Map of predicted value on both == 1 and == 0
test.Data <-
  test.Data %>%
  mutate(SalePrice.Predict = predict(reg1, test.Data))%>%
  st_as_sf()

train.Data.2 <-
  mutate(train.Data.2, Regression = "Baseline Regression")

merge.predicted <-
  train.Data.2 %>%
  select(-SalePrice.Error,-SalePrice.AbsError, -SalePrice.APE, -Regression)

merge.predicted <-
  rbind(merge.predicted,test.Data)

ggplot()+
  geom_sf(data = BoulderCounty_Bundary, fill = "grey50") +
  geom_sf(data = tracts19, colour = "black")+
  geom_sf(data = merge.predicted, aes(colour = q5(SalePrice.Predict)),size=.85)+
  scale_colour_manual(values = palette5,
                      labels=qbr(merge.predicted,"SalePrice.Predict"),
                      name = "Predicted Sale Price")+
  labs(title = "Predicted Sale Prices in Boulder County",
       subtitle = "Bouolder, Colorado ($)",
       caption = "Fig. 10")+
  mapTheme()

```

We used tracts to summarize our absolute percentage error to examine spatial patterns across Boulder County. As shown in Fig. 11, some tracts lack datapoints, and are gray. There is not a clear, visually apparent pattern of error across tracts, other than that our model struggled more in the Longmont area. In our graphical analysis of census tracts in Fig, 11, there is no evident correlation between mean absolute percentage error and mean housing prices either. This shows that census tracts do not create generalized error.

```{r Muni With Error,message = FALSE, results=FALSE, echo=FALSE}
#Census Track with Error
tracts_with_error <-
  st_join(tracts19 ,train.Data.2)%>%
  select(price,GEOID.x,SalePrice.APE,SalePrice.AbsError)

tracts_with_error <-
  tracts_with_error %>%
  group_by(GEOID.x)%>%
  summarize("Mean.absE" = mean(SalePrice.AbsError, na.rm = T),
            "MAPE" = mean(SalePrice.APE, na.rm = T),
            "Mean.price" = mean(price), na.rm = T)%>%
  mutate(MAPE.Pct = MAPE*100)

ggplot()+
  geom_sf(data = BoulderCounty_Bundary, fill = "grey70") +
  geom_sf(data = tracts_with_error, aes(fill = MAPE.Pct),colour = "black")+
  scale_fill_gradient(low = "white", high = "brown",
    name = "Absolute Percentage Error" ) +
  labs(title = "Mean Absolute Error Aggregated Based on Census Tracts", caption = "Fig. 11", subtitle = "Boulder County, Colorado")+
  mapTheme()

ggplot(data = tracts_with_error, aes(Mean.price, MAPE.Pct)) +
  geom_point(size = 1.5,colour = "orange") + 
  labs(x = "Mean Housing Prices",y = "Mean Absolute Percentage Errors", 
       title = "Absolute Percentage Error as a function of Mean Housing Prices by Municipalities",
       subtitle = "Boulder County, Colorado", caption = "Fig. 12")+
  theme(plot.title = element_text(size = 20),
        plot.subtitle = element_text(size = 15),
        axis.title.x = element_text(size = 15),
        axis.title.y = element_text(size = 15),
        axis.line = element_line(colour = "grey50", size = 1),
        panel.grid.major = element_line(linetype = "dotted",size = 1))

left_join(
  st_drop_geometry(train.Data.2) %>%
    group_by(GEOID) %>%
    summarize(meanPrice = mean(price, na.rm = T)),
  mutate(train.Data.2, predict.fe = 
                        predict(lm(price ~ GEOID, data = train.Data.2), 
                        train.Data.2)) %>%
    st_drop_geometry %>%
    group_by(GEOID) %>%
      summarize(meanPrediction = mean(predict.fe)))
```

If we consider census tracts as neighborhoods, we can account for neighborhood effects in our regression model. Using this logic, we add census tract GEOID as one of our indicators along with the previously included variables. 

```{r neighborhood effects, message=FALSE}
reg.nhood <- lm(price ~ ., data = as.data.frame(train.Data.1) %>% 
                                 select(price, GEOID,
                 section_num,
                 qualityCode,
                 TotalFinishedSF,
                 Ac,
                 Heating,
                 Age,
                 med_inc,
                 landmark_dist,
                 nbrRoomsNobath,
                 pvty_pop,
                 privat_dist,
                 head_nn5,
                 park_nn1,
                 Loui_dummy,
                 Nede_dummy,
                 Boul_dummy,
                 Erie_dummy,
                 Long_dummy,
                 Lyon_dummy,
                 pgcount500m,
                 TotalBath,
                 flood_dist,
                 privat_dist))

boulder.test.nhood <-
  train.Data.2 %>%
  mutate(Regression = "Neighborhood Effects",
         SalePrice.Predict = predict(reg.nhood, train.Data.2),
         SalePrice.Error = SalePrice.Predict- price,
         SalePrice.AbsError = abs(SalePrice.Predict- price),
         SalePrice.APE = (abs(SalePrice.Predict- price)) / price)

```


Table 7 compares the absolute error between the baseline regression and a version of that regression that considers neighborhood effects. Based on measures of absolute error and APE, these regressions are very similar.

Fig. 13 maps the relationship between predicted and actual prices for the house prices in the test set. The orange line represents a function in which predicted values perfectly represent the observed values. The green lines represent the regression of the two models. The similarities between the baseline version of the plot and the plot considering neighborhood effects show that the neighborhood effects do not prediction accuracy much. 

```{r, results=FALSE}
bothRegressions <- 
  rbind(select(train.Data.2, starts_with("SalePrice"), Regression, GEOID,price) %>%
 mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)),
select(boulder.test.nhood, starts_with("SalePrice"), Regression, GEOID,price) %>%
  mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)))

st_drop_geometry(bothRegressions) %>%
  gather(Variable, Value, -Regression, -GEOID) %>%
  filter(Variable == "SalePrice.AbsError" | Variable == "SalePrice.APE") %>%
  group_by(Regression, Variable) %>%
    summarize(meanValue = mean(Value, na.rm = T)) %>%
    spread(Variable, meanValue) %>%
    kable(caption = "Table 7")%>% kable_styling()
```



```{r}
bothRegressions %>%
  select(SalePrice.Predict, price, Regression) %>%
    ggplot(aes(price, SalePrice.Predict)) +
  geom_point() +
  stat_smooth(aes(price, price), 
             method = "lm", se = FALSE, size = 1, colour="#FA7800") + 
  stat_smooth(aes(SalePrice.Predict, price), 
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  facet_wrap(~Regression) +
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction", caption = "Fig. 13") +
  plotTheme()
```

The maps of Fig. 14 present geographic representations of demographics in Boulder County. These maps provide a framework for understanding how demographic variables affect sales price, or potential clustering. Every tract in Boulder County is majority white based on 2019 ACS data. For our map of median income across Boulder County census tracts, tracts are designated as "high income" (greater than the county median of $40450), or "low income" (lower than the county median). 

```{r}
tracts19.2 <- 
  tracts19%>%
  mutate(percentWhite = white_pop / tot_pop,
         raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(med_inc > 40450, "High Income", "Low Income"))

grid.arrange(ncol = 2,
  ggplot() + geom_sf(data = na.omit(tracts19.2), aes(fill = raceContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Race Context") +
    labs(title = "Race Context", caption = "Fig. 14") +
    mapTheme() + theme(legend.position="bottom"), 
  ggplot() + geom_sf(data = na.omit(tracts19.2), aes(fill = incomeContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Income Context") +
    labs(title = "Income Context") +
    mapTheme() + theme(legend.position="bottom"))
```

We examine income as a potential neighborhood effect. Table 8 shows a lower MAPE in the low income area than in the high income area. This indicates that at least in low income areas, neighborhood effects do account for the clustering of housing prices. 

```{r}
st_join(bothRegressions, tracts19.2) %>% 
  group_by(Regression, incomeContext) %>%
  summarize(mean.MAPE = scales::percent(mean(SalePrice.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(incomeContext, mean.MAPE) %>%
  kable(caption = "Table 8. Test set MAPE by neighborhood racial context") %>%
  kable_styling()
```
# Conclusion

The model we created was certainly improved by the addition of geographic features. While many variables from the given dataset were also central to the model’s predictions, variables such as distance to the nearest park and the mean distance to the five closest trailheads performed well. 
_more on which variables were important_

Throughout the cyclical iterations of the project’s development our team faced difficult decisions related to acceptance and rejection of given variables.  Ultimately, a mean-absolute-percent-error (MAPE) of around 30% allows for a modest level of confidence in our model.  Given more research and an enhanced qualitative context we are inclined to think that the model has room for improvement.  Geographically, Boulder City had the highest concentration of errors. This error may have been caused by higher density development or the unique real estate environment created by the presence of a large University in the area. The rest of the municipalities had generally more accurate predictions. Overall, the model we created was certainly improved by the addition of geographic features.

Variables used as predictors

```{r Summary Statistics of Predictors used in the model,echo=FALSE, results='asis'}
InterCh.va <-
studentData %>%
   select("Section.Num" = section_num,
          "Quality.Code" = qualityCode,
          TotalFinishedSF,
          Age,
          Ac,
          Heating,
          "Total.Bath" = TotalBath,
          "Rooms.No.bath" = nbrRoomsNobath)%>%
   st_drop_geometry()

 SpacialStruct.va <-
   studentData %>%
   select("Med.Income" = med_inc,
          "White.Pop" = white_pop,
          "Pop.Den" = pop_den,
          "Pvty.Pop" = pvty_pop) %>%
   st_drop_geometry()

 Amen.va <-
   studentData %>%
   select(landmark_dist,
           privat_dist,
            head_nn5,
           park_nn1,
           pgcount500m,
          flood_dist)%>%
   st_drop_geometry()
 
Private.Dist <-
 as.numeric(Amen.va$privat_dist)

LandMark.Dist <-
  as.numeric(Amen.va$landmark_dist)

Flood.Dist <-
  as.numeric(Amen.va$flood_dist)

Amen.va <-
  cbind(Amen.va, Flood.Dist,LandMark.Dist,Private.Dist)%>%
  select(LandMark.Dist,
          Private.Dist,
          "Trailhead.Dist" = head_nn5,
          "Park.Dist" = park_nn1,
          "PGCount.500m" = pgcount500m,
          Flood.Dist)
   

 stargazer(InterCh.va, type = "html", title = "Statistic Summary: Internal Characteristic Variables",
           digits = 0,
           notes = c("Section.Num: Building section number by uses",
           "Quality.Code: Quality as determined by our appraisal staff",
           "TotalFinishedSF: Total number of finished square feet",
           "Age: Age of the structure",
           "Ac: Air Conditioner Type (Lower N value due to absence of AC units)",
           "Heating: Heating System Type (Lower N value due to absence of Heating units)",
           "Total.Bath: Number of Bathrooms",
           "Rooms.No.Bath: Number of Rooms that are not bathrooms"))

 stargazer(Amen.va, type = "html", title = "Statistic Summary: Amenities",
           digits = 0,
           notes = c("Unit: Meters Source: Boulder County Open Data Portal",
             "LandMark.Dist: Distance to the nearest landmark",
          "Private.Dist: Distance to the nearest private school",
          "Trailhead.Dist: Distance to the nearest trail head",
          "Park.Dist: Distance to the nearest park",
          "PGCount.500: Playground count in 500m radius of a sold house",
          "Flood.Dist: Distance to the nearest Floodplain"))

 stargazer(SpacialStruct.va, type = "html", title = "Statistic Summary: Spacial Structure",
           digits = 0,
           notes = c("Source: One-year American Community Survey",
           "Data collected by census tracts",
           "Med.Income: Medium Family Income",
          "White.Pop: Population identified as 'white'",
          "Pop.Den: Population density by census tract",
          "Pvty.Pop: Population living in poverty",
          "Some variables are catagorical. In these variables below",
          "'1' indicates the sold house exists within the municipality,",
          "0 means non-exsiting:",
          "Loui\\_dummy, Ward\\_dummy, Jame\\_dummy, Nede\\_dummy,Boul\\_dummy",
          "Erie\\_dummy,Lafa\\_dummy,Long\\_dummy,",
          "representing Louisvelle, Ward, Jamestown, Nederland, Boulder, Erie,",
          "Lafayette, and Longmont, respectively"))
```